---
title: "Benchmarking data.table"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Benchmarking data.table}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

<style>
h2 {
    font-size: 20px;
}
</style>

This vignette documents the best ways to accurately measure data.table's performance, as well as common traps to avoid.

## `microbenchmark` is not always appropriate
Repetitive benchmarking like that used by `microbenchmark` may not always be appropriate. 
While it makes perfect sense for timing atomic calculations, it may provide misleading results for common data processing tasks, which are typically larger than the scale for which `microbenchmark` was designed, and run only once.

For example, the following gives the impression that `data.table` is an order of magnitude slower than `data.frame` for updating columns.

``` r
library(microbenchmark)
library(data.table)
DF <- data.frame(x = 1:5)
DT <- data.table(x = 1:5)
microbenchmark(DF$y <- seq.int(nrow(DF)), DT[, y := .I])
#> Unit: microseconds
#>                       expr     min      lq      mean   median       uq        max neval cld
#>  DF$y <- seq.int(nrow(DF))   6.927   9.939  14.37229  15.2095  17.4690     35.840   100  a 
#>          DT[, `:=`(y, .I)] 207.510 221.064 276.51600 228.7435 238.0795   4912.183   100   b
```

yet there is typically no real difference:

```r
n = 1e7
DT = data.table( a=sample(1:1000,n,replace=TRUE),
                 b=sample(1:1000,n,replace=TRUE),
                 c=rnorm(n),
                 d=sample(c("foo","bar","baz","qux","quux"),n,replace=TRUE),
                 e=rnorm(n),
                 f=sample(1:1000,n,replace=TRUE) )
DF <- as.data.frame(DT)
system.time(DF$y <- seq.int(nrow(DF)))
#>    user  system elapsed 
#>    0.01    0.00    0.02
system.time(DT[, y := .I])
#>    user  system elapsed 
#>    0.01    0.01    0.03
```

Further, when benchmarking `set*` functions it only makes to measure the first run. Those functions update data.table by reference thus after the first run the data.table involved in the measurement will have changed.

To correctly measure the average performance of `set`, protect your data.table from being updated by reference operations by using `copy` or `data.table:::shallow`. Be aware `copy` might be very expensive relative to the operation you want to time as it duplicates the whole table, but this is what other packages usually do. It is unlikely we want to include duplication time in time of the actual task we are benchmarking. It may be prudent to measure the execution time of `copy` alone and subtract it from the operation with both `copy` and the operation being timed.

Matt once said:

> I'm very wary of benchmarks measured in anything under 1 second. Much prefer 10 seconds or more for a single run, achieved by increasing data size. A repetition count of 500 is setting off alarm bells. 3-5 runs should be enough to convince on larger data. Call overhead and time to GC affect inferences at this very small scale.

## fread: clear caches

Each `fread` call should be run in fresh session. Use the following shell commands before each timing. This clears OS cache file in RAM and HD cache.

```sh
free -g
sudo sh -c 'echo 3 >/proc/sys/vm/drop_caches'
sudo lshw -class disk
sudo hdparm -t /dev/sda
```

## subset: index optimization switch off

Index optimization will currently be turned off when doing subset using index and when cross product of elements provided to filter on exceeds > 1e4.

## subset: index aware benchmarking

For convinience data.table automatically builds index on fields you are doing subset data. It will add some overhead to first subset on particular fields but greatly reduce time to query those columns in subsequent runs. When measuring speed best way is to measure index creation and query using index separately. Having such timings it is easy to decide what is the optimal strategy for your use case.
To control usage of index use following options (see `?datatable.optimize` for more details):

```r
options(datatable.optimize=2L)
options(datatable.optimize=3L)
options(datatable.auto.index=TRUE)
options(datatable.use.index=TRUE)
```
`options(datatable.optimize=2L)` will turn off optimization of subsets completely, while `options(datatable.optimize=3L)` will switch it back on.
`use.index=FALSE` will force query not to use index even if it exists, but existing keys are used for optimization. `auto.index=FALSE` only disables building index automatically when doing subset on non-indexed data.





## multithreaded processing

One of the main factors that is likely to impact timings is the number of threads on your machine. In recent versions of data.table some of the functions has been parallelized.
You can control how much threads you want to use with `setDTthreads`.

## avoid `data.table()` inside a loop

As of now `data.table()` has an overhead, thus inside loops it is preferred to use `as.data.table()` or `setDT()` or maybe even `setattr(<list>, "class", c("data.table", "data.frame"))` on a valid list.
